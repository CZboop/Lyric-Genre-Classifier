{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EVnjCFTG5nqe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "n2mZx7PbD0Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenc = LabelEncoder()"
      ],
      "metadata": {
        "id": "IL25DsXt7kh9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "lGlqtAvfD2XC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snow = SnowballStemmer('english')\n",
        "wnlem = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ZHx3enEuD8Ls"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessor:\n",
        "    def __init__(self, raw_list):\n",
        "        self.raw_list = raw_list\n",
        "        self.lowercased = None\n",
        "        self.sent_tokenized = None\n",
        "        self.word_tokenized = None\n",
        "        self.cleaned_special_chars = None\n",
        "        self.without_stopwords = None\n",
        "        self.stemmed = None\n",
        "        self.lemmed = None\n",
        "\n",
        "    def lowercase(self):\n",
        "        lowercased = []\n",
        "        for text in self.raw_list:\n",
        "            lowercased.append(text.lower())\n",
        "        self.lowercased = lowercased\n",
        "        return self.lowercased\n",
        "\n",
        "    def tokenize_sentence(self):\n",
        "        self.sent_tokenized = [sent_tokenize(sent) for sent in self.lowercased]\n",
        "        return self.sent_tokenized\n",
        "    \n",
        "    def tokenize_word(self):\n",
        "        self.word_tokenized = [word_tokenize(word) for word in self.lowercased]\n",
        "        return self.word_tokenized\n",
        "\n",
        "    def clean_special_chars(self):\n",
        "        cleaned_text = []\n",
        "        for sentence in self.sent_tokenized:\n",
        "          cleaned_sent = []\n",
        "          for word in sentence:\n",
        "            clean = re.sub(r'[^\\w\\s]', \"\", word)\n",
        "            if clean != \"\":\n",
        "              cleaned_sent.append(clean)\n",
        "          cleaned_text.append(cleaned_sent)\n",
        "        self.cleaned_special_chars = cleaned_text\n",
        "        return self.cleaned_special_chars\n",
        "\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        without_stopwords = []\n",
        "        for sentences in self.cleaned_special_chars:\n",
        "            no_stop = []\n",
        "            for words in sentences:\n",
        "              for word in words.split(\" \"):\n",
        "                if word not in stopwords.words('english'):\n",
        "                  no_stop.append(word)\n",
        "            without_stopwords.append(\" \".join(no_stop))\n",
        "        self.without_stopwords = without_stopwords\n",
        "        return self.without_stopwords\n",
        "    \n",
        "    def stem(self):\n",
        "        stemmed_list = []\n",
        "        for elem in [i for i in self.without_stopwords if type(i)!=list]:\n",
        "          stemmed_list.append([snow.stem(word) for word in elem.split(\" \")])\n",
        "        self.stemmed = stemmed_list\n",
        "        return self.stemmed\n",
        "    \n",
        "    def lemmatize(self):\n",
        "        lemmed = []\n",
        "        for item in [i for i in self.without_stopwords if type(i)!=list]:\n",
        "          lemmed.append([wnlem.lemmatize(word) for word in item.split(\" \")])\n",
        "        self.lemmed = lemmed\n",
        "        return self.lemmed\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.lowercase()\n",
        "        self.tokenize_sentence()\n",
        "        self.tokenize_word()\n",
        "        self.clean_special_chars()\n",
        "        self.remove_stopwords()\n",
        "        self.stem()\n",
        "        self.lemmatize()\n"
      ],
      "metadata": {
        "id": "tRsHMyyZEFE9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_df = pd.read_csv('lyrics-data.csv',  usecols=[0,1,3,4], header = None, delimiter=\",\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "lyric_df.columns = ['alink', 'song', 'lyric', 'lang']\n",
        "# for both, data has commas inside which is messing with delimiter, come back to this"
      ],
      "metadata": {
        "id": "-GOrTqrm7cyv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_df.shape"
      ],
      "metadata": {
        "id": "URpfBAMN8VjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artist_df = pd.read_csv('artists-data.csv', usecols=[0,1,3,4,5],header = None, delimiter=\",\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "artist_df.columns = ['artist', 'songs', 'link', 'genre', 'genres']"
      ],
      "metadata": {
        "id": "FkjpIhu0BJYe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artist_df.shape"
      ],
      "metadata": {
        "id": "bLHv5NX3BJgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artist_df.head\n",
        "lyric_df.head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHKwU_fvC5KJ",
        "outputId": "47b83a1e-d1dc-4f96-c946-59b6fe2749d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                    alink  ...                                               lang\n",
              "0                  ALink  ...                                              Idiom\n",
              "1        /10000-maniacs/  ...                                            ENGLISH\n",
              "2        /10000-maniacs/  ...                                               baby\n",
              "3        /10000-maniacs/  ...   I promise. Will the whole world be warm as th...\n",
              "4        /10000-maniacs/  ...   \"\"O my mountain has coal veins and beds to di...\n",
              "...                  ...  ...                                                ...\n",
              "210258  /zeca-pagodinho/  ...                     iaiá. Você me jogou um feitiço\n",
              "210259  /zeca-pagodinho/  ...                         um desejo a mais. Veja bem\n",
              "210260  /zeca-pagodinho/  ...                                              palma\n",
              "210261  /zeca-pagodinho/  ...   cadê a samba?. Está mangando na curimba. Está...\n",
              "210262  /zeca-pagodinho/  ...                                                 \"Ô\n",
              "\n",
              "[210263 rows x 4 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just getting english lyrics and a temporary way of removing some of the dodgy data due to delimiter issue\n",
        "lyric_df = lyric_df.loc[lyric_df['lang'] == 'ENGLISH']\n",
        "\n",
        "# making a list of genres to match up with rows of df that can then add as a column\n",
        "lyrics_genres = []\n",
        "for index, row in lyric_df.iterrows():\n",
        "  # below may be returning a series, currently just string casting may want to handle other way later\n",
        "  main_genre = artist_df.loc[artist_df['link'] == row['alink']]['genre']\n",
        "  lyrics_genres.append(str(main_genre))\n",
        "\n",
        "# something not quite right getting multiple genres for some and none for others it seems\n",
        "\n",
        "# adding genres to lyrics\n",
        "lyric_df['genre'] = lyrics_genres\n"
      ],
      "metadata": {
        "id": "j_kPZNi8CEv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lyric_df.head"
      ],
      "metadata": {
        "id": "a6sG8PtaIoCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_text = lyric_df['lyric']\n",
        "lyric_list = lyric_text.to_list()\n",
        "lyric_processor = PreProcessor(lyric_list)\n",
        "lyric_processor.preprocess()"
      ],
      "metadata": {
        "id": "GdxmKgYmENen"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_lyrics = lyric_processor.stemmed\n",
        "stemmed_joined = [\" \".join(i) for i in stemmed_lyrics]\n",
        "stemmed_series = pd.Series(stemmed_joined)\n",
        "lyric_df.drop(['lyric'], axis=1)\n",
        "lyric_df['lyric'] = stemmed_series\n",
        "lyric_df.head"
      ],
      "metadata": {
        "id": "tIT4m7BDHmX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now that there's genre column will encode to use as class/ifier\n",
        "lyric_data_encoded = lyric_df.apply(lenc.fit_transform, axis=0)\n",
        "lyric_data_encoded1 = lyric_data_encoded.loc[lyric_data_encoded['genre'] == 0]\n",
        "lyric_data_encoded2 = lyric_data_encoded.loc[lyric_data_encoded['genre'] == 1]\n",
        "lyric_data_encoded = pd.concat([lyric_data_encoded1, lyric_data_encoded2])\n",
        "lyric_data_encoded.head\n",
        "# because of genre extraction issues, there are more genres appearing than there should be, may filter out non standard, ignore etc "
      ],
      "metadata": {
        "id": "I_mUsWKMXRXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_lyric_data = lyric_data_encoded.values"
      ],
      "metadata": {
        "id": "Pce3AyTYcJS1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assigning lyrics and genre to info and class for later use\n",
        "lyric_info = encoded_lyric_data[:, 2:4]\n",
        "lyric_classes = encoded_lyric_data[:, -1]"
      ],
      "metadata": {
        "id": "jSZmRkVccP8j"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,  X_test, y_train, y_test = train_test_split(\n",
        "    lyric_info, lyric_classes, test_size=0.20, random_state=75\n",
        "    )"
      ],
      "metadata": {
        "id": "zHv-QS0efu2b"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prior_prob(y, label):\n",
        "  total = y.shape[0]\n",
        "  actual = np.sum(y == label)\n",
        "\n",
        "  return total / actual\n"
      ],
      "metadata": {
        "id": "w4DQl4wsgn9o"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_prob(X_train, y_train, feature_col, feature_val, label):\n",
        "  X_filtered = X_train[y_train == label]\n",
        "  num = np.sum(X_filtered[:, feature_col] == feature_val)\n",
        "  denom = X_filtered.shape[0]\n",
        "\n",
        "  return num/denom\n"
      ],
      "metadata": {
        "id": "c-l0_d6Oh2Nz"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_train, y_train, X_test):\n",
        "  classes = np.unique(y_train)\n",
        "  features = X_train.shape[1]\n",
        "  \n",
        "  posterior_prob = []\n",
        "\n",
        "  for label in classes:\n",
        "    chance = 1.0\n",
        "    for feature in range(features):\n",
        "      cond = conditional_prob(X_train, y_train, feature, X_test[feature], label)\n",
        "      chance = chance * cond \n",
        "    prior = get_prior_prob(y_train, label)\n",
        "    posterior = chance * prior\n",
        "    posterior_prob.append(posterior)\n",
        "\n",
        "    most_likely = np.argmax(posterior_prob)\n",
        "\n",
        "    return most_likely"
      ],
      "metadata": {
        "id": "KFfdpqykjXvV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(X_train, y_train, X_test, y_test):\n",
        "  preds = []\n",
        "  for i in range (X_test.shape[0]):\n",
        "    pred = predict(X_train, y_train, X_test[i])\n",
        "    preds.append(pred)\n",
        "  class_preds = np.array(preds)\n",
        "  \n",
        "  accuracy = np.sum(class_preds == y_test)/ class_preds.shape[0]\n",
        "\n",
        "  return accuracy \n"
      ],
      "metadata": {
        "id": "oNOyK57ClggQ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = get_accuracy(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "58Gw34GRmyjx"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc)"
      ],
      "metadata": {
        "id": "bZ2Pj6r31d2B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}