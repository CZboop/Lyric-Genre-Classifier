{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EVnjCFTG5nqe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "n2mZx7PbD0Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenc = LabelEncoder()"
      ],
      "metadata": {
        "id": "IL25DsXt7kh9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "lGlqtAvfD2XC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snow = SnowballStemmer('english')\n",
        "wnlem = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ZHx3enEuD8Ls"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessor:\n",
        "    def __init__(self, raw_list):\n",
        "        self.raw_list = raw_list\n",
        "        self.lowercased = None\n",
        "        self.sent_tokenized = None\n",
        "        self.word_tokenized = None\n",
        "        self.cleaned_special_chars = None\n",
        "        self.without_stopwords = None\n",
        "        self.stemmed = None\n",
        "        self.lemmed = None\n",
        "\n",
        "    def lowercase(self):\n",
        "        lowercased = []\n",
        "        for text in self.raw_list:\n",
        "            lowercased.append(text.lower())\n",
        "        self.lowercased = lowercased\n",
        "        return self.lowercased\n",
        "\n",
        "    def tokenize_sentence(self):\n",
        "        self.sent_tokenized = [sent_tokenize(sent) for sent in self.lowercased]\n",
        "        return self.sent_tokenized\n",
        "    \n",
        "    def tokenize_word(self):\n",
        "        self.word_tokenized = [word_tokenize(word) for word in self.lowercased]\n",
        "        return self.word_tokenized\n",
        "\n",
        "    def clean_special_chars(self):\n",
        "        cleaned_text = []\n",
        "        for sentence in self.sent_tokenized:\n",
        "          cleaned_sent = []\n",
        "          for word in sentence:\n",
        "            clean = re.sub(r'[^\\w\\s]', \"\", word)\n",
        "            if clean != \"\":\n",
        "              cleaned_sent.append(clean)\n",
        "          cleaned_text.append(cleaned_sent)\n",
        "        self.cleaned_special_chars = cleaned_text\n",
        "        return self.cleaned_special_chars\n",
        "\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        without_stopwords = []\n",
        "        for sentences in self.cleaned_special_chars:\n",
        "            no_stop = []\n",
        "            for words in sentences:\n",
        "              for word in words.split(\" \"):\n",
        "                if word not in stopwords.words('english'):\n",
        "                  no_stop.append(word)\n",
        "            without_stopwords.append(\" \".join(no_stop))\n",
        "        self.without_stopwords = without_stopwords\n",
        "        return self.without_stopwords\n",
        "    \n",
        "    def stem(self):\n",
        "        stemmed_list = []\n",
        "        for elem in [i for i in self.without_stopwords if type(i)!=list]:\n",
        "          stemmed_list.append([snow.stem(word) for word in elem.split(\" \")])\n",
        "        self.stemmed = stemmed_list\n",
        "        return self.stemmed\n",
        "    \n",
        "    def lemmatize(self):\n",
        "        lemmed = []\n",
        "        for item in [i for i in self.without_stopwords if type(i)!=list]:\n",
        "          lemmed.append([wnlem.lemmatize(word) for word in item.split(\" \")])\n",
        "        self.lemmed = lemmed\n",
        "        return self.lemmed\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.lowercase()\n",
        "        self.tokenize_sentence()\n",
        "        self.tokenize_word()\n",
        "        self.clean_special_chars()\n",
        "        self.remove_stopwords()\n",
        "        self.stem()\n",
        "        self.lemmatize()\n"
      ],
      "metadata": {
        "id": "tRsHMyyZEFE9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_df = pd.read_csv('traincommasnewlinesremoved.csv',  usecols=range(0,5), header = None, delimiter=\",\", quoting=csv.QUOTE_NONE, \n",
        "                       encoding='utf-8')\n",
        "lyric_df.columns = ['artist', 'song', 'genre', 'lang', 'lyrics']"
      ],
      "metadata": {
        "id": "-GOrTqrm7cyv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_df.shape\n",
        "lyric_df.head"
      ],
      "metadata": {
        "id": "URpfBAMN8VjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starting to standardise the column numbers here, \n",
        "#will get the longest lyrics (pink floyd - the wall, it seems) and pad the rest with zeros to be same length\n",
        "lyric_df['lyrics']=lyric_df['lyrics'].fillna(\"\")\n",
        "lyrics = lyric_df['lyrics'].tolist()\n",
        "max_lyric_len = max([i.split(\" \") for i in lyrics], key=len)"
      ],
      "metadata": {
        "id": "B-1V2q0aqEMD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_col_num = len(max_lyric_len) #longest lyric "
      ],
      "metadata": {
        "id": "ed8AhjfBqqf9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_split_lyrics = []\n",
        "# uses too much ram going thru in one go\n",
        "for index, row in lyric_df.iloc[:100000].iterrows():\n",
        "  padded_list = [0] * word_col_num \n",
        "  list_of_lyrics = row['lyrics'].split(\" \")\n",
        "  padded_list[:len(list_of_lyrics)] = list_of_lyrics\n",
        "  padded_split_lyrics.append(padded_list)\n",
        "for index, row in lyric_df.iloc[100000:200000].iterrows():\n",
        "  padded_list = [0] * word_col_num \n",
        "  list_of_lyrics = row['lyrics'].split(\" \")\n",
        "  padded_list[:len(list_of_lyrics)] = list_of_lyrics\n",
        "  padded_split_lyrics.append(padded_list)\n",
        "for index, row in lyric_df.iloc[200000:].iterrows():\n",
        "  padded_list = [0] * word_col_num \n",
        "  list_of_lyrics = row['lyrics'].split(\" \")\n",
        "  padded_list[:len(list_of_lyrics)] = list_of_lyrics\n",
        "  padded_split_lyrics.append(padded_list)\n",
        "print(padded_split_lyrics[-10:]) "
      ],
      "metadata": {
        "id": "DJMOqGKgrEqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding the split words as columns to the dataframe\n",
        "lyric_cols_array = np.array(padded_split_lyrics)\n",
        "print(lyric_cols_array[:,:10])\n",
        "lyric_cols_array = lyric_cols_array.transpose()\n",
        "print(lyric_cols_array[0])\n",
        "# lol i guess some of the lyrics are guitar tabs? can maybe filter those out"
      ],
      "metadata": {
        "id": "Sp9_pb-MhMWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c,v in enumerate(lyric_cols_array):\n",
        "  lyric_df['lyrics'+str(c)] = v.tolist()\n",
        "\n",
        "lyric_df.lyrics0.head"
      ],
      "metadata": {
        "id": "RNfFe1RViXi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_df = lyric_df.astype(str)\n",
        "print(lyric_df['genre'])"
      ],
      "metadata": {
        "id": "YjKnJWGtZbwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not preprocessing atm, need to remove stopwords at least, or may end up using only key words"
      ],
      "metadata": {
        "id": "GdxmKgYmENen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_data = lyric_df.apply(lenc.fit_transform, axis=0)\n"
      ],
      "metadata": {
        "id": "I_mUsWKMXRXB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df = encoded_data.values\n",
        "# leaving out for now i think"
      ],
      "metadata": {
        "id": "Pce3AyTYcJS1"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assigning lyrics and genre to info and class for later use, lyrics will become everything past the other columns rather than a single one\n",
        "lyric_info = encoded_df[:, 5:]\n",
        "lyric_classes = encoded_df[:, 2]"
      ],
      "metadata": {
        "id": "jSZmRkVccP8j"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,  X_test, y_train, y_test = train_test_split(\n",
        "    lyric_info, lyric_classes, test_size=0.20, random_state=75\n",
        "    )"
      ],
      "metadata": {
        "id": "zHv-QS0efu2b"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train\n",
        "type(X_test)"
      ],
      "metadata": {
        "id": "vzc9SKq0ludS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prior_prob(y, label):\n",
        "  total = y.shape[0]\n",
        "  actual = np.sum(y == label)\n",
        "\n",
        "  return total / actual\n"
      ],
      "metadata": {
        "id": "w4DQl4wsgn9o"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_prob(X_train, y_train, feature_col, feature_val, label):\n",
        "  X_filtered = X_train[y_train == label]\n",
        "  num = np.sum(X_filtered[:, feature_col] == feature_val)\n",
        "  denom = X_filtered.shape[0]\n",
        "\n",
        "  return num/denom\n"
      ],
      "metadata": {
        "id": "c-l0_d6Oh2Nz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_train, y_train, X_test):\n",
        "  classes = np.unique(y_train)\n",
        "  features = X_train.shape[1]\n",
        "  \n",
        "  posterior_prob = []\n",
        "\n",
        "  for label in classes:\n",
        "    chance = 1.0\n",
        "    for feature in range(features):\n",
        "      cond = conditional_prob(X_train, y_train, feature, X_test[feature], label)\n",
        "      chance = chance * cond \n",
        "    prior = get_prior_prob(y_train, label)\n",
        "    posterior = chance * prior\n",
        "    posterior_prob.append(posterior)\n",
        "\n",
        "    most_likely = np.argmax(posterior_prob)\n",
        "\n",
        "    return most_likely"
      ],
      "metadata": {
        "id": "KFfdpqykjXvV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(X_train, y_train, X_test, y_test):\n",
        "  preds = []\n",
        "  for i in range (X_test.shape[0]):\n",
        "    pred = predict(X_train, y_train, X_test[i])\n",
        "    preds.append(pred)\n",
        "  class_preds = np.array(preds)\n",
        "  \n",
        "  accuracy = np.sum(class_preds == y_test)/ class_preds.shape[0]\n",
        "\n",
        "  return accuracy \n"
      ],
      "metadata": {
        "id": "oNOyK57ClggQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = get_accuracy(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "58Gw34GRmyjx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc)"
      ],
      "metadata": {
        "id": "bZ2Pj6r31d2B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}