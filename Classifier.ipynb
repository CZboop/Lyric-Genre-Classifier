{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EVnjCFTG5nqe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "n2mZx7PbD0Pf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1f189e-64a3-4975-fb79-fd48b4f1ade2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lenc = LabelEncoder()"
      ],
      "metadata": {
        "id": "IL25DsXt7kh9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "lGlqtAvfD2XC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snow = SnowballStemmer('english')\n",
        "wnlem = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ZHx3enEuD8Ls"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessor:\n",
        "    def __init__(self, raw_list):\n",
        "        self.raw_list = raw_list\n",
        "        self.lowercased = None\n",
        "        self.sent_tokenized = None\n",
        "        self.word_tokenized = None\n",
        "        self.cleaned_special_chars = None\n",
        "        self.without_stopwords = None\n",
        "        self.stemmed = None\n",
        "        self.lemmed = None\n",
        "\n",
        "    def lowercase(self):\n",
        "        lowercased = []\n",
        "        for text in self.raw_list:\n",
        "            lowercased.append(text.lower())\n",
        "        self.lowercased = lowercased\n",
        "        return self.lowercased\n",
        "\n",
        "    def tokenize_sentence(self):\n",
        "        self.sent_tokenized = [sent_tokenize(sent) for sent in self.lowercased]\n",
        "        return self.sent_tokenized\n",
        "    \n",
        "    def tokenize_word(self):\n",
        "        self.word_tokenized = [word_tokenize(word) for word in self.lowercased]\n",
        "        return self.word_tokenized\n",
        "\n",
        "    def clean_special_chars(self):\n",
        "        cleaned_text = []\n",
        "        for sentence in self.sent_tokenized:\n",
        "          cleaned_sent = []\n",
        "          for word in sentence:\n",
        "            clean = re.sub(r'[^\\w\\s]', \"\", word)\n",
        "            if clean != \"\":\n",
        "              cleaned_sent.append(clean)\n",
        "          cleaned_text.append(cleaned_sent)\n",
        "        self.cleaned_special_chars = cleaned_text\n",
        "        return self.cleaned_special_chars\n",
        "\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        without_stopwords = []\n",
        "        for sentences in self.cleaned_special_chars:\n",
        "            no_stop = []\n",
        "            for words in sentences:\n",
        "              for word in words.split(\" \"):\n",
        "                if word not in stopwords.words('english'):\n",
        "                  no_stop.append(word)\n",
        "            without_stopwords.append(\" \".join(no_stop))\n",
        "        self.without_stopwords = without_stopwords\n",
        "        return self.without_stopwords\n",
        "    \n",
        "    def stem(self):\n",
        "        stemmed_list = []\n",
        "        for elem in [i for i in self.without_stopwords if type(i)!=list]:\n",
        "          stemmed_list.append([snow.stem(word) for word in elem.split(\" \")])\n",
        "        self.stemmed = stemmed_list\n",
        "        return self.stemmed\n",
        "    \n",
        "    def lemmatize(self):\n",
        "        lemmed = []\n",
        "        for item in [i for i in self.without_stopwords if type(i)!=list]:\n",
        "          lemmed.append([wnlem.lemmatize(word) for word in item.split(\" \")])\n",
        "        self.lemmed = lemmed\n",
        "        return self.lemmed\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.lowercase()\n",
        "        self.tokenize_sentence()\n",
        "        self.tokenize_word()\n",
        "        self.clean_special_chars()\n",
        "        self.remove_stopwords()\n",
        "        self.stem()\n",
        "        self.lemmatize()\n"
      ],
      "metadata": {
        "id": "tRsHMyyZEFE9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_df = pd.read_csv('traincommasnewlinesremoved.csv',  usecols=range(0,5), header = None, delimiter=\",\", quoting=csv.QUOTE_NONE, \n",
        "                       encoding='utf-8')\n",
        "lyric_df.columns = ['artist', 'song', 'genre', 'lang', 'lyrics']"
      ],
      "metadata": {
        "id": "-GOrTqrm7cyv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_df.shape\n",
        "lyric_df.head"
      ],
      "metadata": {
        "id": "URpfBAMN8VjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starting to standardise the column numbers here, \n",
        "#will get the longest lyrics (pink floyd - the wall, it seems) and pad the rest with zeros to be same length\n",
        "lyric_df['lyrics']=lyric_df['lyrics'].fillna(\"\")\n",
        "lyrics = lyric_df['lyrics'].tolist()\n",
        "max_lyric_len = max([i.split(\" \") for i in lyrics], key=len)"
      ],
      "metadata": {
        "id": "B-1V2q0aqEMD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_col_num = len(max_lyric_len) #longest lyric "
      ],
      "metadata": {
        "id": "ed8AhjfBqqf9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_split_lyrics = []\n",
        "# uses too much ram going thru in one go\n",
        "for index, row in lyric_df.iloc[:100000].iterrows():\n",
        "  padded_list = [0] * word_col_num \n",
        "  list_of_lyrics = row['lyrics'].split(\" \")\n",
        "  padded_list[:len(list_of_lyrics)] = list_of_lyrics\n",
        "  padded_split_lyrics.append(padded_list)\n",
        "for index, row in lyric_df.iloc[100000:200000].iterrows():\n",
        "  padded_list = [0] * word_col_num \n",
        "  list_of_lyrics = row['lyrics'].split(\" \")\n",
        "  padded_list[:len(list_of_lyrics)] = list_of_lyrics\n",
        "  padded_split_lyrics.append(padded_list)\n",
        "# print(padded_split_lyrics[10]) "
      ],
      "metadata": {
        "id": "DJMOqGKgrEqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lyric_df2 = pd.read_csv('testcommasnewlinesremoved.csv', usecols=[0,2,3,4],header = None, delimiter=\",\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "# lyric_df2.columns = ['Song', 'Artist', 'Genre', 'Lyrics']\n",
        "# the dataset is already split into files intended for train and test, may initially combine and train test split using sklearn\n",
        "# just working with first df for now, data also formatted slightly differently"
      ],
      "metadata": {
        "id": "FkjpIhu0BJYe"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_df = lyric_df.astype(str)"
      ],
      "metadata": {
        "id": "YjKnJWGtZbwv"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lyric_text = lyric_df['lyrics']\n",
        "# lyric_list = lyric_text.to_list()\n",
        "# lyric_processor = PreProcessor(lyric_list)\n",
        "# lyric_processor.preprocess()\n",
        "# may return to preprocessing, atm may not be that useful"
      ],
      "metadata": {
        "id": "GdxmKgYmENen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemmed_lyrics = lyric_processor.stemmed\n",
        "# stemmed_joined = [\" \".join(i) for i in stemmed_lyrics]\n",
        "# stemmed_series = pd.Series(stemmed_joined)\n",
        "# lyric_df.drop(['lyrics'], axis=1)\n",
        "# lyric_df['lyrics'] = stemmed_series"
      ],
      "metadata": {
        "id": "tIT4m7BDHmX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now that there's genre column will encode to use as class/ifier\n",
        "lyric_data_encoded = lyric_df.apply(lenc.fit_transform, axis=0)\n",
        "lyric_df['genre'] = lenc.fit_transform(lyric_df['genre'])\n",
        "lyric_df.genre.values\n"
      ],
      "metadata": {
        "id": "I_mUsWKMXRXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d51ec5-6b68-4ac1-e332-22719428609c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8, 15, 15, ..., 13, 10, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_lyric_data = lyric_data_encoded.values\n",
        "# leaving out for now i think"
      ],
      "metadata": {
        "id": "Pce3AyTYcJS1"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assigning lyrics and genre to info and class for later use, lyrics will become everything past the other columns rather than a single one\n",
        "lyric_info = lyric_df.iloc[:, 4:5]\n",
        "lyric_classes = lyric_df.iloc[:, 2]"
      ],
      "metadata": {
        "id": "jSZmRkVccP8j"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,  X_test, y_train, y_test = train_test_split(\n",
        "    lyric_info, lyric_classes, test_size=0.20, random_state=75\n",
        "    )"
      ],
      "metadata": {
        "id": "zHv-QS0efu2b"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "vzc9SKq0ludS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prior_prob(y, label):\n",
        "  total = y.shape[0]\n",
        "  actual = np.sum(y == label)\n",
        "\n",
        "  return total / actual\n"
      ],
      "metadata": {
        "id": "w4DQl4wsgn9o"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_prob(X_train, y_train, feature_col, feature_val, label):\n",
        "  X_filtered = X_train[y_train == label]\n",
        "  num = np.sum(X_filtered[:, feature_col] == feature_val)\n",
        "  denom = X_filtered.shape[0]\n",
        "\n",
        "  return num/denom\n"
      ],
      "metadata": {
        "id": "c-l0_d6Oh2Nz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_train, y_train, X_test):\n",
        "  classes = np.unique(y_train)\n",
        "  features = X_train.shape[1]\n",
        "  \n",
        "  posterior_prob = []\n",
        "\n",
        "  for label in classes:\n",
        "    chance = 1.0\n",
        "    for feature in range(features):\n",
        "      cond = conditional_prob(X_train, y_train, feature, X_test[feature], label)\n",
        "      chance = chance * cond \n",
        "    prior = get_prior_prob(y_train, label)\n",
        "    posterior = chance * prior\n",
        "    posterior_prob.append(posterior)\n",
        "\n",
        "    most_likely = np.argmax(posterior_prob)\n",
        "\n",
        "    return most_likely"
      ],
      "metadata": {
        "id": "KFfdpqykjXvV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(X_train, y_train, X_test, y_test):\n",
        "  preds = []\n",
        "  for i in range (X_test.shape[0]):\n",
        "    pred = predict(X_train, y_train, X_test[i])\n",
        "    preds.append(pred)\n",
        "  class_preds = np.array(preds)\n",
        "  \n",
        "  accuracy = np.sum(class_preds == y_test)/ class_preds.shape[0]\n",
        "\n",
        "  return accuracy \n"
      ],
      "metadata": {
        "id": "oNOyK57ClggQ"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = get_accuracy(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "58Gw34GRmyjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc)"
      ],
      "metadata": {
        "id": "bZ2Pj6r31d2B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}